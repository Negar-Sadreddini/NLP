{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "snscrape Twitter Scrapping.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Twitter Scrapping**"
      ],
      "metadata": {
        "id": "lVpVWKD2VWD7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yvjGU1HFT-I3"
      },
      "outputs": [],
      "source": [
        "!pip install -q snscrape==0.3.4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import date\n",
        "import itertools\n",
        "import snscrape.modules.twitter as sntwitter"
      ],
      "metadata": {
        "id": "ru6OWA0nT_uX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Creating a dataframe containing information of British Columbia cities**"
      ],
      "metadata": {
        "id": "ZdR4RW5sVIc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bc_cities = pd.read_html('https://en.wikipedia.org/wiki/List_of_cities_in_British_Columbia')\n",
        "\n",
        "BritishColumbia = pd.DataFrame(bc_cities[0])\n",
        "BritishColumbia.drop(53,inplace=True)\n",
        "BritishColumbia.head(3)\n",
        "\n",
        "BritishColumbia['Name'].replace({'Vancouver[a]':'Vancouver','Victoria[b]':'Victoria'},inplace=True)"
      ],
      "metadata": {
        "id": "HU7_WrY9kAao"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "keyword = '(pizza OR covid)'\n",
        "\n",
        "general_dict = {}\n",
        "\n",
        "for idx,city in enumerate(BritishColumbia['Name']):\n",
        "  txt = 'wellbeing near:\"'+city+'\" within:{}km'.format(np.sqrt(BritishColumbia['Area (km2)[5]'].iloc[idx]))\n",
        "  general_dict[city] = pd.DataFrame(itertools.islice(sntwitter.TwitterSearchScraper(txt).get_items(), 10))"
      ],
      "metadata": {
        "id": "c3TmoBBzgYM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keyword = '(pizza OR covid OR USA)'    ###### If we want to scrape multiple keywords#######\n",
        "key_words_wellbeing = ['welfare','health','wellbeing','mental illness','mental health','physical health','physical illness','psychiatrist','health issues']\n",
        "search_key = '('\n",
        "for el in key_words_wellbeing:\n",
        "    search_key += el+' OR '\n",
        "\n",
        "search_key = search_key[:-4]+')'\n",
        "search_key"
      ],
      "metadata": {
        "id": "qmMV17ehw33b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyword = '(pizza OR covid OR USA)'    ###### If we want to scrapp multiple keywords#######\n",
        "\n",
        "\n",
        "import time\n",
        "from datetime import timedelta\n",
        "start_time = time.monotonic()\n",
        "\n",
        "my_dict = {}\n",
        "for idx,city in enumerate(BritishColumbia['Name']):\n",
        "  tweets = []\n",
        "  tdf = None\n",
        "  for i,tweet in enumerate(sntwitter.TwitterSearchScraper('(wellbeing OR health) near:\"'+city+'\" within:{}km'.format(np.sqrt(BritishColumbia['Area (km2)[5]'].iloc[idx]))).get_items()) :\n",
        "    if i > 10000 :\n",
        "              break\n",
        "    text = tweet.content\n",
        "    pubdate = tweet.date\n",
        "    tweets.append({\n",
        "      \"date\":pubdate,\n",
        "      \"content\":text,\n",
        "    })\n",
        "    my_dict[city]=tweets\n",
        "  \n",
        "for key in my_dict.keys():\n",
        "  my_dict[key] = pd.DataFrame(my_dict[key])\n",
        "\n",
        "end_time = time.monotonic()\n",
        "print('Runtime is:',timedelta(seconds=end_time - start_time))"
      ],
      "metadata": {
        "id": "2B6VXz9klk6v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05c0896e-1f82-4973-9654-0e6c2b3ec48a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Runtime is: 0:25:09.169407\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key in my_dict.keys():\n",
        "  my_dict[key] = pd.DataFrame(my_dict[key])\n",
        "  \n",
        "for k,v in my_dict.items():\n",
        "  print(k)\n",
        "  print(len(v))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnx7ecI8OOna",
        "outputId": "b44ccc52-655d-41e6-df6b-54f4d4cb717f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Abbotsford\n",
            "673\n",
            "Burnaby\n",
            "1001\n",
            "Campbell River\n",
            "242\n",
            "Castlegar\n",
            "337\n",
            "Chilliwack\n",
            "961\n",
            "Colwood\n",
            "1001\n",
            "Coquitlam\n",
            "1001\n",
            "Courtenay\n",
            "767\n",
            "Cranbrook\n",
            "169\n",
            "Dawson Creek\n",
            "52\n",
            "Delta\n",
            "1001\n",
            "Duncan\n",
            "89\n",
            "Enderby\n",
            "1001\n",
            "Fernie\n",
            "61\n",
            "Fort St. John\n",
            "266\n",
            "Grand Forks\n",
            "695\n",
            "Greenwood\n",
            "91\n",
            "Kamloops\n",
            "1001\n",
            "Kelowna\n",
            "1001\n",
            "Kimberley\n",
            "293\n",
            "Langford\n",
            "1001\n",
            "Langley\n",
            "161\n",
            "Maple Ridge\n",
            "1001\n",
            "Merritt\n",
            "46\n",
            "Mission\n",
            "1001\n",
            "Nanaimo\n",
            "610\n",
            "Nelson\n",
            "53\n",
            "New Westminster\n",
            "1001\n",
            "North Vancouver\n",
            "1001\n",
            "Parksville\n",
            "98\n",
            "Penticton\n",
            "752\n",
            "Pitt Meadows\n",
            "1001\n",
            "Port Alberni\n",
            "257\n",
            "Port Coquitlam\n",
            "1001\n",
            "Port Moody\n",
            "1001\n",
            "Powell River\n",
            "94\n",
            "Prince George\n",
            "1001\n",
            "Prince Rupert\n",
            "62\n",
            "Quesnel\n",
            "366\n",
            "Revelstoke\n",
            "99\n",
            "Richmond\n",
            "1001\n",
            "Rossland\n",
            "30\n",
            "Salmon Arm\n",
            "135\n",
            "Surrey\n",
            "1001\n",
            "Terrace\n",
            "64\n",
            "Trail\n",
            "72\n",
            "Vancouver\n",
            "1001\n",
            "Vernon\n",
            "888\n",
            "Victoria\n",
            "1001\n",
            "West Kelowna\n",
            "1001\n",
            "White Rock\n",
            "313\n",
            "Williams Lake\n",
            "567\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new dictionary based on tweets after 2019:\n",
        "\n",
        "my_dict_covid = {}\n",
        "\n",
        "for key,val in my_dict.items():  \n",
        "  my_dict_covid[key] = val[val['date']>'2019-01-01']"
      ],
      "metadata": {
        "id": "kB3Ki7HfwOJ_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Creating the dataframe to perform NLP:\n",
        "\n",
        "date = []\n",
        "content = []\n",
        "city = []\n",
        "\n",
        "for k,v in my_dict_covid.items():\n",
        "  for dat in my_dict_covid[k].date:\n",
        "    date.append(dat)\n",
        "  for cnt in my_dict_covid[k].content:\n",
        "    content.append(cnt)\n",
        "    city.append(k)\n",
        "\n",
        "nlp_df = pd.DataFrame(list(zip(date,content,city)),columns=['Date','Content','City'])"
      ],
      "metadata": {
        "id": "ey95H5ECYlxW"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preposessing NLP dataframe:"
      ],
      "metadata": {
        "id": "pNP2rZK7d9lC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_df['Content'] = nlp_df['Content'].str.lower()\n",
        "\n",
        "nlp_df['Content'].duplicated().sum() #There is a good chance that we have duplicated rows:\n",
        "nlp_df.drop_duplicates(subset =\"Content\" ,inplace=True)"
      ],
      "metadata": {
        "id": "UADMZJIx_e5j"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key_words_seniors = ['elderl','senior','old','aged','aging']\n",
        "\n",
        "# Number of scrapping results that contain key_words_seniors elements:\n",
        "nlp_df['Content'].apply(lambda x: any(item in x for item in key_words_seniors)).sum()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pceuVIuE_urf",
        "outputId": "cff60cfe-2aa8-49dd-e899-b1abfbd0f4bb"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "871"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_df.loc[nlp_df['Content'].apply(lambda x: any(item in x for item in key_words_seniors))]['Content']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "av21krQoqhvW",
        "outputId": "389cbce1-c2f2-4542-b73b-061bf7cbe021"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5        @jaymeekitch8 @ctvnews agree except it needs t...\n",
              "6        hey guys! did you know water buffalo milk is e...\n",
              "9        @arthister câ€™mon dr. art. lighten up a little....\n",
              "23       @johnwrightlive well said!! the virus is no lo...\n",
              "37       @nnulk there many. icbc policy changes, improv...\n",
              "                               ...                        \n",
              "22723    all i told him was that the vaccine ðŸ’‰ passport...\n",
              "22744    @ketaminh you borrowed it for safe keeping. ðŸ˜‰\\...\n",
              "22767    also just a random post.i feel like our suppor...\n",
              "22772    i told my dad i was annoyed that i only heard ...\n",
              "22777    @cbsnews scott is in the pocket of the health ...\n",
              "Name: Content, Length: 871, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key_words_seniors = ['elderl','senior','old','aged','aging']\n",
        "\n",
        "# Processing \n",
        "def list_rm(lst,item):\n",
        "  lst.remove(item)\n",
        "  return lst\n",
        "\n",
        "idx = []\n",
        "for word in key_words_seniors:\n",
        "  list_rm(key_words_seniors,word)\n",
        "  for line,content in enumerate(nlp_df['Content']):\n",
        "    if word in content:\n",
        "      if content[content.index(word)-1] != ' ' and  not any(item in content for item in key_words_seniors):\n",
        "        print(line)\n",
        "        #print(word)\n",
        "        #print(content)\n",
        "        #nlp_df.drop(line , axis=0 , inplace=True)\n",
        "        idx.append(line)\n",
        "  key_words_seniors.append(word)\n",
        "\n",
        "nlp_df.drop(nlp_df.index[list(set(idx))],inplace=True)"
      ],
      "metadata": {
        "id": "mSJ-XK9BIA7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeWMrlMDvZ9h",
        "outputId": "5abc270a-35b7-4e49-a281-f10b6fd381d4"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15142, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key_words_seniors\n",
        "\n",
        "key_words_wellbeing = ['welfare','mental illness','mental health','physical health','physical illness','psychiatrist','health issues']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5HffOCGLE_6",
        "outputId": "e0a09974-44ce-4318-eb93-245bb3180a9b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['senior', 'aged', 'elderl', 'aging', 'old']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key_words_wellbeing = ['mental illness','mental health','physical health','physical illness','psychiatrist','health issues']\n",
        "search_key = '('\n",
        "for el in key_words_wellbeing:\n",
        "    search_key += el+' OR '\n",
        "search_key = search_key[:-4]+')'\n",
        "search_key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ySnxAPUP1juu",
        "outputId": "f19368df-51ea-4dfc-bee6-66de68185bc8"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'(mental illness OR mental health OR physical health OR physical illness OR psychiatrist OR health issues)'"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount('drive')\n",
        "\n",
        "  nlp_df.to_csv('NLP_Data.csv')\n",
        "  !cp NLP_Data.csv \"drive/My Drive/\"\n",
        "except:\n",
        "  (\"It's not google colab!\")"
      ],
      "metadata": {
        "id": "5bGCSffQrGxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Twitters within Vancouver area"
      ],
      "metadata": {
        "id": "t0hqCOidfGCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"loc_centre = '54.15, -126.54, 10km' #Coordinates of the center of british columbia\n",
        "\n",
        "loc = '49.246292, -123.116226, 60km'\n",
        "df_coord = pd.DataFrame(itertools.islice(sntwitter.TwitterSearchScraper(\n",
        "    'wellbeing geocode:\"{}\"'.format(loc)).get_items(), 10000))[['date', 'content']]\n",
        "\n",
        "# Extracting tweets after year 2019:\n",
        "df_coord = df_coord[df_coord['date']>'2019-01-01']\"\"\""
      ],
      "metadata": {
        "id": "kIDWIzf1d78Y"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}